{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e928ff20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8a7a4497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              time  linear_acceleration_x  \\\n",
      "0    2015-01-01 06:00:00.000000000               0.020421   \n",
      "1    2015-01-01 06:00:00.100000000               0.021195   \n",
      "2    2015-01-01 06:00:00.300000000               0.020522   \n",
      "3    2015-01-01 06:00:00.410000128               0.019842   \n",
      "4    2015-01-01 06:00:00.600000000               0.021527   \n",
      "...                            ...                    ...   \n",
      "1442 2015-01-01 06:03:36.200000000               0.266655   \n",
      "1443 2015-01-01 06:03:36.400000000               0.292025   \n",
      "1444 2015-01-01 06:03:36.500000000              -0.019332   \n",
      "1445 2015-01-01 06:03:36.710000128               1.857258   \n",
      "1446 2015-01-01 06:03:36.900000000              -0.299269   \n",
      "\n",
      "      linear_acceleration_y  linear_acceleration_z  position_x  \n",
      "0                 -0.119444              -9.806449  -79.782379  \n",
      "1                 -0.104146              -9.805132  -79.782379  \n",
      "2                 -0.119607              -9.806278  -79.782379  \n",
      "3                 -0.120573              -9.805198  -79.782379  \n",
      "4                 -0.107108              -9.806895  -79.782379  \n",
      "...                     ...                    ...         ...  \n",
      "1442               0.356157              -9.770982  -79.782369  \n",
      "1443               0.466108              -9.771317  -79.782371  \n",
      "1444               0.182839              -9.813704  -79.782372  \n",
      "1445               0.564382              -8.907895  -79.782373  \n",
      "1446               0.543802              -9.631512  -79.782375  \n",
      "\n",
      "[1447 rows x 5 columns]\n",
      "                              time  linear_acceleration_x  \\\n",
      "1447 2015-01-01 06:03:37.000000000              -1.134028   \n",
      "1448 2015-01-01 06:03:37.100000000              -0.400447   \n",
      "1449 2015-01-01 06:03:37.300000000               0.268790   \n",
      "1450 2015-01-01 06:03:37.500000000               0.861068   \n",
      "1451 2015-01-01 06:03:37.600000000              -0.140671   \n",
      "...                            ...                    ...   \n",
      "1804 2015-01-01 06:04:30.700000000              -0.097605   \n",
      "1805 2015-01-01 06:04:30.809999872              -0.093623   \n",
      "1806 2015-01-01 06:04:31.000000000              -0.095287   \n",
      "1807 2015-01-01 06:04:31.100000000              -0.096167   \n",
      "1808 2015-01-01 06:04:31.200000000              -0.098171   \n",
      "\n",
      "      linear_acceleration_y  linear_acceleration_z  position_x  \n",
      "1447               0.443106             -10.912999  -79.782375  \n",
      "1448               0.356869             -10.296550  -79.782376  \n",
      "1449               0.404308             -10.231118  -79.782377  \n",
      "1450               0.323070              -9.543383  -79.782378  \n",
      "1451               0.283928              -9.856499  -79.782378  \n",
      "...                     ...                    ...         ...  \n",
      "1804              -0.116972              -9.806151  -79.782394  \n",
      "1805              -0.109916              -9.808828  -79.782394  \n",
      "1806              -0.117011              -9.808269  -79.782394  \n",
      "1807              -0.103729              -9.807520  -79.782394  \n",
      "1808              -0.119673              -9.805540  -79.782395  \n",
      "\n",
      "[362 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "uav = pd.read_csv('uav_flight_data.csv')\n",
    "\n",
    "# TODO: Read the datetime from the parameters file\n",
    "start = dt.datetime(year=2015,month=1,day=1)\n",
    "uav['time'] = uav['time'] + start.timestamp()\n",
    "uav[\"time\"] = pd.to_datetime(uav[\"time\"], unit='s')\n",
    "\n",
    "uav = uav[['time', 'linear_acceleration_x', 'linear_acceleration_y', 'linear_acceleration_z', 'position_x']]\n",
    "\n",
    "# Do I still need this for this clean dataset?\n",
    "uav.fillna(uav.bfill())\n",
    "\n",
    "# Extract the training and test datasets\n",
    "train = uav[:int(len(uav)*0.8)]\n",
    "test = uav[int(len(uav)*0.8):]\n",
    "\n",
    "train_dates = pd.to_datetime(train['time'])\n",
    "test_dates  = pd.to_datetime(test['time'])\n",
    "\n",
    "print(train)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c9ec3485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.48897588 0.43692092 0.52463933 0.28559377]\n",
      " [0.48915698 0.44051114 0.52482616 0.28558124]\n",
      " [0.48899932 0.43688277 0.52466363 0.28547282]\n",
      " ...\n",
      " [0.4796729  0.50786142 0.52361    0.29010891]\n",
      " [0.91882679 0.59740246 0.65213127 0.28911476]\n",
      " [0.4141629  0.59257283 0.54946038 0.28833141]]\n",
      "1447\n"
     ]
    }
   ],
   "source": [
    "# Prepare Data\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "scaled_data = scaler.fit_transform(train[['linear_acceleration_x', 'linear_acceleration_y', 'linear_acceleration_z', 'position_x']])\n",
    "\n",
    "prediction_days = 225\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for x in range(prediction_days, len(scaled_data)):\n",
    "    x_train.append(scaled_data[x-prediction_days:x, 0])\n",
    "    y_train.append(scaled_data[x, 0])\n",
    "\n",
    "\n",
    "x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb8c0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build The Model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(units =128, activation='tanh', return_sequences=True, input_shape = (x_train.shape[1],1)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(units =128, activation='tanh', return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(units =128, activation='tanh', return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units=1)) # Prediction of the next value\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "model.summary()\n",
    "\n",
    "## Need to figure out how the data size and the batch_size work ... need to optimize for a GPU system.  Higher batch size works much better on the GPU (less overhead)\n",
    "history = model.fit(x_train, y_train, epochs = 25, batch_size=32, validation_split=0.1)\n",
    "\n",
    "\n",
    "plt.plot(history.history['loss'], label = 'Training Loss')\n",
    "plt.plot(history.history['val_loss'], label = 'Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6ef32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test the Model Accuracy on Existing Data ###\n",
    "################################################\n",
    "\n",
    "# take Test data Again\n",
    "test.head()\n",
    "actual_temp = test['Avg_Temp'].values\n",
    "total_temp = pd.concat((train['Avg_Temp'], test['Avg_Temp']),axis=0)\n",
    "\n",
    "model_inputs = total_temp[len(total_temp)-len(test)-prediction_days:].values\n",
    "model_inputs = model_inputs.reshape(-1,1)\n",
    "model_inputs = scaler.transform(model_inputs)\n",
    "\n",
    "\n",
    "# Make Predictions on Test Data\n",
    "x_test = []\n",
    "\n",
    "for x in range(prediction_days, len(model_inputs)):\n",
    "    x_test.append(model_inputs[x-prediction_days:x, 0])\n",
    "\n",
    "x_test = np.array(x_test)\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))\n",
    "\n",
    "pred = model.predict(x_test)\n",
    "print('Predictions:\\n', pred[:][:10])\n",
    "pred = scaler.inverse_transform(pred)\n",
    "print('Inverse Transformed Predictions\\n', pred[:][:10])\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "mean_absolute_error(test['Avg_Temp'], pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b53fdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Visualization ###\n",
    "#####################\n",
    "pred_ = pd.DataFrame(test['Date'])\n",
    "pred_['Avg_Temp'] = pred\n",
    "pred_[\"Date\"] = pd.to_datetime(pred_[\"Date\"])\n",
    "\n",
    "pred_\n",
    "original = florida.loc[florida['Date'] >= '1990-01-01']\n",
    "\n",
    "import seaborn as sns\n",
    "sns.lineplot(original['Date'], original['Avg_Temp'])\n",
    "sns.lineplot(pred_['Date'], pred_['Avg_Temp'])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
